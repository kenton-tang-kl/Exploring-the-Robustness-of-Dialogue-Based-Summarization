{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Robust Text Summarization",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kenton-tang-kl/Exploring-the-Robustness-of-Dialogue-Based-Summarization/blob/main/Robust_Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Project Setup"
      ],
      "metadata": {
        "id": "ARnZ6EcBA4H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate rouge-score py7zr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:22:32.238497Z",
          "iopub.execute_input": "2024-09-11T00:22:32.238841Z",
          "iopub.status.idle": "2024-09-11T00:22:52.015866Z",
          "shell.execute_reply.started": "2024-09-11T00:22:32.238809Z",
          "shell.execute_reply": "2024-09-11T00:22:52.014934Z"
        },
        "trusted": true,
        "id": "y0GAREATA4H-",
        "outputId": "4b99f305-b3a9-455b-b5cd-77714b64c78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.44.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.21.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py7zr\n  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.24.6)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nCollecting pycryptodomex>=3.16.0 (from py7zr)\n  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.15.9 (from py7zr)\n  Downloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting pyppmd<1.2.0,>=1.1.0 (from py7zr)\n  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\nCollecting pybcj<1.1.0,>=1.0.0 (from py7zr)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nCollecting inflate64<1.1.0,>=1.0.0 (from py7zr)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m413.8/413.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=7232bd3e813e109adb6954d1da0415acfad59573c5ac61ec77ccccfc5d9ad865\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, rouge-score, py7zr, evaluate\nSuccessfully installed evaluate-0.4.2 inflate64-1.0.0 multivolumefile-0.2.3 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.20.0 pyppmd-1.1.0 pyzstd-0.16.1 rouge-score-0.1.2\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:22:52.01801Z",
          "iopub.execute_input": "2024-09-11T00:22:52.018407Z",
          "iopub.status.idle": "2024-09-11T00:22:55.235376Z",
          "shell.execute_reply.started": "2024-09-11T00:22:52.018362Z",
          "shell.execute_reply": "2024-09-11T00:22:55.234353Z"
        },
        "trusted": true,
        "id": "6qcAWykCA4IA",
        "outputId": "614b309f-47f3-4a79-ab45-b5d1d847627e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "device(type='cuda')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preparation"
      ],
      "metadata": {
        "id": "0y7sbu_nA4IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "samsum_dataset = load_dataset(\"samsum\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:22:58.063361Z",
          "iopub.execute_input": "2024-09-11T00:22:58.064312Z",
          "iopub.status.idle": "2024-09-11T00:23:06.25885Z",
          "shell.execute_reply.started": "2024-09-11T00:22:58.064273Z",
          "shell.execute_reply": "2024-09-11T00:23:06.257942Z"
        },
        "trusted": true,
        "id": "7ghxPrfaA4IA",
        "outputId": "46e01ec2-fc7b-4c6f-8f7a-2f2e4b07a5e4",
        "colab": {
          "referenced_widgets": [
            "8542a765ab1b443f82cc2aa96c92b804",
            "6bdff0dbd83044e7a21c9ebd3b4ea414",
            "8d6a10975e3a4c288aa9dad639e0211b",
            "b14abc3d0d064e0c8d91fd7a65528def",
            "6ea7890084654048a2bbe73b5f8849c9",
            "e16a6fd2ab214fedadc5e2d6204d8166"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8542a765ab1b443f82cc2aa96c92b804"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6bdff0dbd83044e7a21c9ebd3b4ea414"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d6a10975e3a4c288aa9dad639e0211b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b14abc3d0d064e0c8d91fd7a65528def"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ea7890084654048a2bbe73b5f8849c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e16a6fd2ab214fedadc5e2d6204d8166"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:06.260391Z",
          "iopub.execute_input": "2024-09-11T00:23:06.260772Z",
          "iopub.status.idle": "2024-09-11T00:23:06.267538Z",
          "shell.execute_reply.started": "2024-09-11T00:23:06.260735Z",
          "shell.execute_reply": "2024-09-11T00:23:06.266543Z"
        },
        "trusted": true,
        "id": "WKnbOydmA4IB",
        "outputId": "8364cdeb-909f-4780-b0da-95779d308c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_dataset['test'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:09.41195Z",
          "iopub.execute_input": "2024-09-11T00:23:09.412708Z",
          "iopub.status.idle": "2024-09-11T00:23:09.421111Z",
          "shell.execute_reply.started": "2024-09-11T00:23:09.412663Z",
          "shell.execute_reply": "2024-09-11T00:23:09.420118Z"
        },
        "trusted": true,
        "id": "COVWT-sYA4IB",
        "outputId": "31c15c83-9139-4415-b45f-03144ab0ff77"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'id': '13862856',\n 'dialogue': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nHannah: <file_gif>\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nHannah: <file_gif>\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him ğŸ™‚\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\",\n 'summary': \"Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\"}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_dataset = load_dataset(\"EdinburghNLP/xsum\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:12.605285Z",
          "iopub.execute_input": "2024-09-11T00:23:12.605687Z",
          "iopub.status.idle": "2024-09-11T00:23:21.971725Z",
          "shell.execute_reply.started": "2024-09-11T00:23:12.605645Z",
          "shell.execute_reply": "2024-09-11T00:23:21.970794Z"
        },
        "trusted": true,
        "id": "NYoKCJo1A4IB",
        "outputId": "d6d65d7d-e637-4bd3-ca76-f91f0c1143e4",
        "colab": {
          "referenced_widgets": [
            "39e4685f6b404355b2a40ec0594dca5c",
            "f9d1bc84f64d4e6480823b6f1c6aed24",
            "a6a74b0a10df47a785a50657691cc118",
            "b18dade5bc944ee4b00d5f0f09637859",
            "f43ef00dc7ef456f915d391025b776f9",
            "db972a51863543f28aa21a38d1e70091"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/304M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39e4685f6b404355b2a40ec0594dca5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/16.7M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9d1bc84f64d4e6480823b6f1c6aed24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/17.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6a74b0a10df47a785a50657691cc118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b18dade5bc944ee4b00d5f0f09637859"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f43ef00dc7ef456f915d391025b776f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db972a51863543f28aa21a38d1e70091"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:24.570498Z",
          "iopub.execute_input": "2024-09-11T00:23:24.570883Z",
          "iopub.status.idle": "2024-09-11T00:23:24.576901Z",
          "shell.execute_reply.started": "2024-09-11T00:23:24.570847Z",
          "shell.execute_reply": "2024-09-11T00:23:24.576027Z"
        },
        "trusted": true,
        "id": "J5JklzSmA4IB",
        "outputId": "d4efc829-6e62-4929-aa82-4704f1dcd615"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 204045\n    })\n    validation: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11332\n    })\n    test: Dataset({\n        features: ['document', 'summary', 'id'],\n        num_rows: 11334\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_dataset['test'][0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:28.068977Z",
          "iopub.execute_input": "2024-09-11T00:23:28.069958Z",
          "iopub.status.idle": "2024-09-11T00:23:28.076223Z",
          "shell.execute_reply.started": "2024-09-11T00:23:28.069914Z",
          "shell.execute_reply": "2024-09-11T00:23:28.075347Z"
        },
        "trusted": true,
        "id": "VJs2THxeA4IC",
        "outputId": "068d8485-1fc8-4716-e0af-87cfee1c2ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'document': 'Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\\nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\\nThe Welsh Government said more people than ever were getting help to address housing problems.\\nChanges to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\\nPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\\nHowever, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\\nAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\\n\"There\\'s a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\\n\"It could take six months to a year, without a lot of help they could be on the streets for six months.\\n\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\\nMr Stevens believes building more one-bedroom flats could help ease the problem.\\n\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\\nOfficial figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\\nMarc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\\nHe said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\\n\"You\\'re put out among the same sort of people doing the same sort of thing, and it\\'s difficult, it\\'s difficult to get away from it. It\\'s like every man for himself, there\\'s nothing.\"\\nMarc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\\n\"You feel safe, you got hot food, you\\'ve got company of people in similar situations to yourself but all dealing with different issues. It\\'s a constructive, helpful atmosphere,\" he said.\\nTom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\\n\"We do still see [people] homeless on the streets, so clearly they haven\\'t got accommodation and haven\\'t got provision,\" he said.\\n\"I think the key is connecting people with the services they need. I don\\'t delude myself that Emmaus can offer a one size fits all for everyone, we can\\'t.\\n\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\\nA Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\\nIt added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.',\n 'summary': 'There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.',\n 'id': '38264402'}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(samsum_dataset['train'].filter(lambda example: example['dialogue'] is None or example['summary'] is None))\n",
        "print(xsum_dataset['test'].filter(lambda example: example['document'] is None or example['summary'] is None))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:31.393212Z",
          "iopub.execute_input": "2024-09-11T00:23:31.393893Z",
          "iopub.status.idle": "2024-09-11T00:23:31.665647Z",
          "shell.execute_reply.started": "2024-09-11T00:23:31.393849Z",
          "shell.execute_reply": "2024-09-11T00:23:31.664784Z"
        },
        "trusted": true,
        "id": "jejnlKpkA4IC",
        "outputId": "238610ad-926a-4ef8-f5cf-c484eb0906b4",
        "colab": {
          "referenced_widgets": [
            "652d183882cb4fcf8dd063de0b27b9b0",
            "c9aef2f0543e4aa080040908278b904f"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/14732 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "652d183882cb4fcf8dd063de0b27b9b0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['id', 'dialogue', 'summary'],\n    num_rows: 0\n})\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/11334 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9aef2f0543e4aa080040908278b904f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset({\n    features: ['document', 'summary', 'id'],\n    num_rows: 0\n})\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train = samsum_dataset['train']\n",
        "samsum_val = samsum_dataset['validation']\n",
        "samsum_test = samsum_dataset['test']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:34.232843Z",
          "iopub.execute_input": "2024-09-11T00:23:34.233266Z",
          "iopub.status.idle": "2024-09-11T00:23:34.238355Z",
          "shell.execute_reply.started": "2024-09-11T00:23:34.233223Z",
          "shell.execute_reply": "2024-09-11T00:23:34.237118Z"
        },
        "trusted": true,
        "id": "_d8F6Cq-A4IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_test = xsum_dataset['test']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:36.681661Z",
          "iopub.execute_input": "2024-09-11T00:23:36.682075Z",
          "iopub.status.idle": "2024-09-11T00:23:36.686576Z",
          "shell.execute_reply.started": "2024-09-11T00:23:36.682035Z",
          "shell.execute_reply": "2024-09-11T00:23:36.685652Z"
        },
        "trusted": true,
        "id": "WCI-RV0xA4IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:39.098401Z",
          "iopub.execute_input": "2024-09-11T00:23:39.098905Z",
          "iopub.status.idle": "2024-09-11T00:23:40.8513Z",
          "shell.execute_reply.started": "2024-09-11T00:23:39.098853Z",
          "shell.execute_reply": "2024-09-11T00:23:40.850297Z"
        },
        "trusted": true,
        "id": "apgZiZ03A4IC",
        "outputId": "81bc7681-b740-4150-bb94-d0830fe7c515",
        "colab": {
          "referenced_widgets": [
            "603bfb539a64428e9ccd0e35b24b39d3",
            "62aa33d4b7874a078922a51d4534d195",
            "2696ad8627c941c8948087ace1b1b6e1",
            "cafe06c54ae248afbaa2842831aad63b"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "603bfb539a64428e9ccd0e35b24b39d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62aa33d4b7874a078922a51d4534d195"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2696ad8627c941c8948087ace1b1b6e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cafe06c54ae248afbaa2842831aad63b"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(example, input_key):\n",
        "    inputs = tokenizer(example[input_key], max_length=1024, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "    targets = tokenizer(example['summary'], max_length=128, truncation=True, padding='max_length', return_tensors=\"pt\")\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'].squeeze(),\n",
        "        'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "        'labels': targets['input_ids'].squeeze()\n",
        "    }"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:43.875615Z",
          "iopub.execute_input": "2024-09-11T00:23:43.876857Z",
          "iopub.status.idle": "2024-09-11T00:23:43.8834Z",
          "shell.execute_reply.started": "2024-09-11T00:23:43.876809Z",
          "shell.execute_reply": "2024-09-11T00:23:43.882099Z"
        },
        "trusted": true,
        "id": "iNYZki6KA4IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train = samsum_train.map(lambda x: preprocess_function(x, 'dialogue'), batched=True)\n",
        "samsum_val = samsum_val.map(lambda x: preprocess_function(x, 'dialogue'), batched=True)\n",
        "samsum_test = samsum_test.map(lambda x: preprocess_function(x, 'dialogue'), batched=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:23:46.196941Z",
          "iopub.execute_input": "2024-09-11T00:23:46.197322Z",
          "iopub.status.idle": "2024-09-11T00:24:33.200063Z",
          "shell.execute_reply.started": "2024-09-11T00:23:46.197285Z",
          "shell.execute_reply": "2024-09-11T00:24:33.199095Z"
        },
        "trusted": true,
        "id": "64JeyXk9A4IC",
        "outputId": "68981f30-ad15-4268-b5c1-4de4c98c64e5",
        "colab": {
          "referenced_widgets": [
            "1f42c19925e84c84822bbd6d7448ba8c",
            "08ddfbbe260345878a0383ef5f287cda",
            "c9ce3db6eb7b46e8a6d47495f8148240"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f42c19925e84c84822bbd6d7448ba8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/818 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08ddfbbe260345878a0383ef5f287cda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9ce3db6eb7b46e8a6d47495f8148240"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_test = xsum_test.map(lambda x: preprocess_function(x, 'document'), batched=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:24:33.201947Z",
          "iopub.execute_input": "2024-09-11T00:24:33.202626Z",
          "iopub.status.idle": "2024-09-11T00:25:38.004875Z",
          "shell.execute_reply.started": "2024-09-11T00:24:33.202577Z",
          "shell.execute_reply": "2024-09-11T00:25:38.003889Z"
        },
        "trusted": true,
        "id": "RiTpBVkIA4ID",
        "outputId": "13239fc2-8156-4aa5-a7e2-cadd6e262f47",
        "colab": {
          "referenced_widgets": [
            "7916c05c886642588aaf41b93665e13e"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/11334 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7916c05c886642588aaf41b93665e13e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "samsum_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "samsum_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "xsum_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:25:38.006001Z",
          "iopub.execute_input": "2024-09-11T00:25:38.006309Z",
          "iopub.status.idle": "2024-09-11T00:25:38.015532Z",
          "shell.execute_reply.started": "2024-09-11T00:25:38.006276Z",
          "shell.execute_reply": "2024-09-11T00:25:38.014506Z"
        },
        "trusted": true,
        "id": "enBs6u4EA4ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_test_subset = xsum_test.select(range(819))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:25:38.019173Z",
          "iopub.execute_input": "2024-09-11T00:25:38.019563Z",
          "iopub.status.idle": "2024-09-11T00:25:38.226749Z",
          "shell.execute_reply.started": "2024-09-11T00:25:38.019531Z",
          "shell.execute_reply": "2024-09-11T00:25:38.225772Z"
        },
        "trusted": true,
        "id": "xD-VLQttA4ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Baseline Evaluation"
      ],
      "metadata": {
        "id": "JOxIrz76A4ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-12-6\").to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:25:38.227887Z",
          "iopub.execute_input": "2024-09-11T00:25:38.228174Z",
          "iopub.status.idle": "2024-09-11T00:25:46.888908Z",
          "shell.execute_reply.started": "2024-09-11T00:25:38.228143Z",
          "shell.execute_reply": "2024-09-11T00:25:46.888096Z"
        },
        "trusted": true,
        "id": "3G61QPnnA4ID",
        "outputId": "8accbc25-265d-474a-9161-b0dc84ddbbb2",
        "colab": {
          "referenced_widgets": [
            "f269ff1b08694ed0944014c578456cce"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f269ff1b08694ed0944014c578456cce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(example):\n",
        "    inputs = example['input_ids'].unsqueeze(0).to(device)\n",
        "    attention_mask = example['attention_mask'].unsqueeze(0).to(device)\n",
        "\n",
        "    summary_ids = model.generate(inputs, attention_mask=attention_mask, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return {\"generated_summary\": summary}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:25:46.889977Z",
          "iopub.execute_input": "2024-09-11T00:25:46.890459Z",
          "iopub.status.idle": "2024-09-11T00:25:46.896207Z",
          "shell.execute_reply.started": "2024-09-11T00:25:46.890413Z",
          "shell.execute_reply": "2024-09-11T00:25:46.895305Z"
        },
        "trusted": true,
        "id": "c1xqcDhuA4ID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_test = samsum_test.map(generate_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:25:46.89723Z",
          "iopub.execute_input": "2024-09-11T00:25:46.897677Z",
          "iopub.status.idle": "2024-09-11T00:36:30.304355Z",
          "shell.execute_reply.started": "2024-09-11T00:25:46.897633Z",
          "shell.execute_reply": "2024-09-11T00:36:30.303443Z"
        },
        "trusted": true,
        "id": "Mf7DSAO-A4ID",
        "outputId": "f3868b4c-2a03-4d12-c850-a745cd985c56",
        "colab": {
          "referenced_widgets": [
            "71fce983e1a84c54807181a5ca64a072"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71fce983e1a84c54807181a5ca64a072"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_test_subset = xsum_test_subset.map(generate_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:36:30.306139Z",
          "iopub.execute_input": "2024-09-11T00:36:30.306464Z",
          "iopub.status.idle": "2024-09-11T00:46:57.45599Z",
          "shell.execute_reply.started": "2024-09-11T00:36:30.306416Z",
          "shell.execute_reply": "2024-09-11T00:46:57.454974Z"
        },
        "trusted": true,
        "id": "LL-HoAbIA4ID",
        "outputId": "1db42dc8-34e8-45f6-962a-613983c55fcb",
        "colab": {
          "referenced_widgets": [
            "182c20c210df4f5a95ff961d111c0c82"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "182c20c210df4f5a95ff961d111c0c82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dialogue: \\n\", samsum_dataset['test'][0]['dialogue'])\n",
        "print(\"Original: \", samsum_dataset['test'][0]['summary'])\n",
        "print(\"Generated: \", samsum_test[0]['generated_summary'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:46:57.457398Z",
          "iopub.execute_input": "2024-09-11T00:46:57.457881Z",
          "iopub.status.idle": "2024-09-11T00:46:57.46594Z",
          "shell.execute_reply.started": "2024-09-11T00:46:57.457836Z",
          "shell.execute_reply": "2024-09-11T00:46:57.465029Z"
        },
        "trusted": true,
        "id": "8i_SIWgSA4ID",
        "outputId": "f95610dd-a4fa-4c54-98b3-e66fb019ff72"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Dialogue: \n Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ğŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\nOriginal:  Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\nGenerated:   Hannah: Hey, do you have Betty's number? I don't know him well. He called her last time we were at the park together. Â  Â  Â  Â  Â  Â  Â  Â Amanda: Ask Larry. The last time I've seen Betty, he's very nice to me. Â  Â  Â  Â  Â  Â  Â  Â  Amanda: Don't be shy. He's a very nice guy. He is very nice.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Document: \\n\", xsum_dataset['test'][0]['document'])\n",
        "print(\"Original: \", xsum_dataset['test'][0]['summary'])\n",
        "print(\"Generated: \", xsum_test_subset[0]['generated_summary'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:46:57.469423Z",
          "iopub.execute_input": "2024-09-11T00:46:57.469798Z",
          "iopub.status.idle": "2024-09-11T00:46:57.487149Z",
          "shell.execute_reply.started": "2024-09-11T00:46:57.469762Z",
          "shell.execute_reply": "2024-09-11T00:46:57.486143Z"
        },
        "trusted": true,
        "id": "OX8J9gSzA4IE",
        "outputId": "93bfc26a-028a-4f64-ce40-6f191586f22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Document: \n Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation.\nWorkers at the charity claim investment in housing would be cheaper than jailing homeless repeat offenders.\nThe Welsh Government said more people than ever were getting help to address housing problems.\nChanges to the Housing Act in Wales, introduced in 2015, removed the right for prison leavers to be given priority for accommodation.\nPrison Link Cymru, which helps people find accommodation after their release, said things were generally good for women because issues such as children or domestic violence were now considered.\nHowever, the same could not be said for men, the charity said, because issues which often affect them, such as post traumatic stress disorder or drug dependency, were often viewed as less of a priority.\nAndrew Stevens, who works in Welsh prisons trying to secure housing for prison leavers, said the need for accommodation was \"chronic\".\n\"There's a desperate need for it, finding suitable accommodation for those leaving prison there is just a lack of it everywhere,\" he said.\n\"It could take six months to a year, without a lot of help they could be on the streets for six months.\n\"When you think of the consequences of either being on the street, especially with the cold weather at the moment or you may have a roof over your head, sometimes there is only one choice.\"\nMr Stevens believes building more one-bedroom flats could help ease the problem.\n\"The average price is a hundred pounds a week to keep someone in a rented flat, prison is a lot more than that so I would imagine it would save the public purse quite a few pounds,\" he said.\nOfficial figures show 830 one-bedroom properties were built in the year to March 2016, of an overall total of 6,900 new properties in Wales.\nMarc, 50, who has been in and out of prison for the past 20 years for burglary offences, said he struggled to find accommodation each time he was released.\nHe said he would ask himself: \"Where am I going to stay? Where am I going to live? Have I got somewhere where I can see my daughter.\"\n\"You're put out among the same sort of people doing the same sort of thing, and it's difficult, it's difficult to get away from it. It's like every man for himself, there's nothing.\"\nMarc has now found stable accommodation with homeless charity Emmaus and said it had been life changing.\n\"You feel safe, you got hot food, you've got company of people in similar situations to yourself but all dealing with different issues. It's a constructive, helpful atmosphere,\" he said.\nTom Clarke, chief executive of Emmaus South Wales, agreed there was not enough support available.\n\"We do still see [people] homeless on the streets, so clearly they haven't got accommodation and haven't got provision,\" he said.\n\"I think the key is connecting people with the services they need. I don't delude myself that Emmaus can offer a one size fits all for everyone, we can't.\n\"But there must be other opportunities and given suitable encouragement I believe that can and should happen.\"\nA Welsh Government spokesman said the national pathway for homeless services to children, young people and adults in the secure estate had prevented many people from losing their home whilst serving their prison sentence.\nIt added there were already significant demands for one-bedroom flats across the public and private sector and it was providing 20,000 new affordable homes in the next five years.\nOriginal:  There is a \"chronic\" need for more housing for prison leavers in Wales, according to a charity.\nGenerated:   Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation. The Welsh Government said more people than ever were getting help to address housing problems. Prisoner Marc, 50, has been in and out of prison for the past 20 years for burglary offences.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "The tendency of distilbart-cnn-12-6 to prioritize the initial or topic sentences when summarizing is well-suited for news articles, where the most important information is often presented upfront. However, this approach can lead to less effective summaries for dialogues, where the key information might be spread throughout the conversation rather than at the beginning."
      ],
      "metadata": {
        "id": "HBBvnGXcA4IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "rouge = load_metric('rouge')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:46:57.488602Z",
          "iopub.execute_input": "2024-09-11T00:46:57.4893Z",
          "iopub.status.idle": "2024-09-11T00:47:01.418401Z",
          "shell.execute_reply.started": "2024-09-11T00:46:57.489263Z",
          "shell.execute_reply": "2024-09-11T00:47:01.417644Z"
        },
        "trusted": true,
        "id": "kCEM05ELA4IE",
        "outputId": "0278c90a-23de-4933-8090-3c13d4938386",
        "colab": {
          "referenced_widgets": [
            "7aa4989dc9a544b5af21e1529de24dcb"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_62/3688894951.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  rouge = load_metric('rouge')\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aa4989dc9a544b5af21e1529de24dcb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_rouge_scores = rouge.compute(predictions=samsum_test[\"generated_summary\"], references=samsum_test[\"summary\"])\n",
        "samsum_rouge_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:47:01.419599Z",
          "iopub.execute_input": "2024-09-11T00:47:01.420301Z",
          "iopub.status.idle": "2024-09-11T00:47:03.193258Z",
          "shell.execute_reply.started": "2024-09-11T00:47:01.42025Z",
          "shell.execute_reply": "2024-09-11T00:47:03.192238Z"
        },
        "trusted": true,
        "id": "YA9Od8-wA4IE",
        "outputId": "99c2b273-5ec9-445e-cb5f-89d4020973f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 25,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.19852457819821803, recall=0.5188277656594559, fmeasure=0.2741439248118298), mid=Score(precision=0.20483311155351527, recall=0.5311703338904473, fmeasure=0.28123381849264706), high=Score(precision=0.21143262976688673, recall=0.5440751384781168, fmeasure=0.2885093925677263)),\n 'rouge2': AggregateScore(low=Score(precision=0.06773379413729942, recall=0.1827648144326898, fmeasure=0.09364599659874502), mid=Score(precision=0.07201805861042407, recall=0.19380442609893866, fmeasure=0.09911068428491915), high=Score(precision=0.07639766131120364, recall=0.20540839939003608, fmeasure=0.10455220555433374)),\n 'rougeL': AggregateScore(low=Score(precision=0.14799006954422336, recall=0.3975048880773811, fmeasure=0.20604603955137418), mid=Score(precision=0.15300673375332763, recall=0.4090583462568975, fmeasure=0.2116341996844942), high=Score(precision=0.15822335421060707, recall=0.42072995068413677, fmeasure=0.21720324266344446)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.1480105640981832, recall=0.3971079406137339, fmeasure=0.20584230974076773), mid=Score(precision=0.15273484122560851, recall=0.409036721262263, fmeasure=0.21134645892874776), high=Score(precision=0.15863188172754483, recall=0.42057847057020364, fmeasure=0.21774672303241024))}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_rouge_scores = rouge.compute(predictions=xsum_test_subset[\"generated_summary\"], references=xsum_test_subset[\"summary\"])\n",
        "xsum_rouge_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:47:03.194612Z",
          "iopub.execute_input": "2024-09-11T00:47:03.19497Z",
          "iopub.status.idle": "2024-09-11T00:47:05.038467Z",
          "shell.execute_reply.started": "2024-09-11T00:47:03.194931Z",
          "shell.execute_reply": "2024-09-11T00:47:05.037512Z"
        },
        "trusted": true,
        "id": "v5U74vJWA4IE",
        "outputId": "78aa40b3-0721-42ac-b8c5-59047ebc1dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'rouge1': AggregateScore(low=Score(precision=0.13809242829735938, recall=0.34039129316972755, fmeasure=0.1940320564045655), mid=Score(precision=0.1419369388321084, recall=0.3490022629282209, fmeasure=0.19887930742362098), high=Score(precision=0.14566128669170147, recall=0.3573691478284609, fmeasure=0.20395243836428395)),\n 'rouge2': AggregateScore(low=Score(precision=0.02236879631041316, recall=0.05633766801378598, fmeasure=0.031492951491069315), mid=Score(precision=0.024351006398248734, recall=0.06144613125112401, fmeasure=0.034299928064285154), high=Score(precision=0.026390355269889462, recall=0.06676934632215757, fmeasure=0.03722454866855688)),\n 'rougeL': AggregateScore(low=Score(precision=0.0903229556841106, recall=0.2234105485896511, fmeasure=0.12689450998755156), mid=Score(precision=0.09279964735857701, recall=0.22915599332281536, fmeasure=0.1300982338504887), high=Score(precision=0.09506844264959954, recall=0.23483599133171948, fmeasure=0.1332288348532887)),\n 'rougeLsum': AggregateScore(low=Score(precision=0.09020953047719198, recall=0.22325250437221705, fmeasure=0.12664479224351147), mid=Score(precision=0.09278555966343988, recall=0.22928154739179882, fmeasure=0.1301616137188443), high=Score(precision=0.09539903805785523, recall=0.2353783558227101, fmeasure=0.1335561838204413))}"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Model Fine-Tuning"
      ],
      "metadata": {
        "id": "VUzO0-agA4IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "samsum_dataset = load_dataset(\"samsum\")\n",
        "xsum_dataset = load_dataset(\"EdinburghNLP/xsum\")\n",
        "samsum_train = samsum_dataset['train']\n",
        "samsum_val = samsum_dataset['validation']\n",
        "samsum_test = samsum_dataset['test']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:47:27.566953Z",
          "iopub.execute_input": "2024-09-11T00:47:27.567355Z",
          "iopub.status.idle": "2024-09-11T00:47:29.635569Z",
          "shell.execute_reply.started": "2024-09-11T00:47:27.567315Z",
          "shell.execute_reply": "2024-09-11T00:47:29.634699Z"
        },
        "trusted": true,
        "id": "UGVT3Lg9A4IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer\n",
        "\n",
        "tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples['dialogue'], max_length=1024, truncation=True)\n",
        "    targets = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'labels': targets['input_ids']\n",
        "    }\n",
        "\n",
        "tokenized_samsum_train = samsum_train.map(preprocess_function, batched=True)\n",
        "tokenized_samsum_val = samsum_val.map(preprocess_function, batched=True)\n",
        "tokenized_samsum_test = samsum_test.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:47:30.365874Z",
          "iopub.execute_input": "2024-09-11T00:47:30.366768Z",
          "iopub.status.idle": "2024-09-11T00:48:05.323256Z",
          "shell.execute_reply.started": "2024-09-11T00:47:30.366724Z",
          "shell.execute_reply": "2024-09-11T00:48:05.322479Z"
        },
        "trusted": true,
        "id": "bFjW5vBHA4IE",
        "outputId": "5fa61449-353e-4f7c-8960-9302f1945f3d",
        "colab": {
          "referenced_widgets": [
            "ac672cd356c0480f884c7149f8ff0cd0",
            "13cd47f6294b4414b982c07035a9c28f",
            "55e466e75c9b44f9ac70e69d1382ccdb"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac672cd356c0480f884c7149f8ff0cd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/818 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13cd47f6294b4414b982c07035a9c28f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55e466e75c9b44f9ac70e69d1382ccdb"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_xsum_function(examples):\n",
        "    inputs = tokenizer(examples['document'], max_length=1024, truncation=True)\n",
        "    targets = tokenizer(examples['summary'], max_length=128, truncation=True)\n",
        "    return {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        'labels': targets['input_ids']\n",
        "    }\n",
        "\n",
        "tokenized_xsum_test = xsum_test.map(preprocess_xsum_function, batched=True)\n",
        "tokenized_xsum_test_subset = tokenized_xsum_test.select(range(819))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:48:40.700106Z",
          "iopub.execute_input": "2024-09-11T00:48:40.700864Z",
          "iopub.status.idle": "2024-09-11T00:49:39.972811Z",
          "shell.execute_reply.started": "2024-09-11T00:48:40.700822Z",
          "shell.execute_reply": "2024-09-11T00:49:39.972023Z"
        },
        "trusted": true,
        "id": "p6R_x4aJA4IF",
        "outputId": "e0e2cb70-c31c-4e20-aace-1df0847b48f0",
        "colab": {
          "referenced_widgets": [
            "f0346c3a204e4e95ac8a17927f61ba29"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/11334 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0346c3a204e4e95ac8a17927f61ba29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:49:42.103568Z",
          "iopub.execute_input": "2024-09-11T00:49:42.104021Z",
          "iopub.status.idle": "2024-09-11T00:49:52.97962Z",
          "shell.execute_reply.started": "2024-09-11T00:49:42.103977Z",
          "shell.execute_reply": "2024-09-11T00:49:52.978795Z"
        },
        "trusted": true,
        "id": "RR9MJbn1A4IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.02,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    fp16=True,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:49:52.98115Z",
          "iopub.execute_input": "2024-09-11T00:49:52.981837Z",
          "iopub.status.idle": "2024-09-11T00:49:54.098999Z",
          "shell.execute_reply.started": "2024-09-11T00:49:52.9818Z",
          "shell.execute_reply": "2024-09-11T00:49:54.098232Z"
        },
        "trusted": true,
        "id": "DUcqJxPDA4IF",
        "outputId": "1c59d8f9-1706-4458-ef30-71aad74c889b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_samsum_train,\n",
        "    eval_dataset=tokenized_samsum_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:50:01.047248Z",
          "iopub.execute_input": "2024-09-11T00:50:01.047934Z",
          "iopub.status.idle": "2024-09-11T00:50:02.153014Z",
          "shell.execute_reply.started": "2024-09-11T00:50:01.047891Z",
          "shell.execute_reply": "2024-09-11T00:50:02.152249Z"
        },
        "trusted": true,
        "id": "I0ZM4wi2A4IF",
        "outputId": "bb0a8bd2-71f8-48db-82dd-071feb8fcc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T00:50:04.757314Z",
          "iopub.execute_input": "2024-09-11T00:50:04.75819Z",
          "iopub.status.idle": "2024-09-11T02:01:46.765044Z",
          "shell.execute_reply.started": "2024-09-11T00:50:04.758147Z",
          "shell.execute_reply": "2024-09-11T02:01:46.764251Z"
        },
        "trusted": true,
        "id": "QIlIKoqfA4IF",
        "outputId": "cbcd0aa9-dc95-4b47-a975-09f730518cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.17.9 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.17.7"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240911_005016-q8s0h4rr</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface/runs/q8s0h4rr' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface' target=\"_blank\">https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface/runs/q8s0h4rr' target=\"_blank\">https://wandb.ai/kentontang0202-university-of-edinburgh/huggingface/runs/q8s0h4rr</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2763' max='2763' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2763/2763 1:11:11, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.646500</td>\n      <td>1.437786</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.290500</td>\n      <td>1.411307</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.116300</td>\n      <td>1.422624</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "execution_count": 33,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=2763, training_loss=1.331983082371262, metrics={'train_runtime': 4300.8965, 'train_samples_per_second': 10.276, 'train_steps_per_second': 0.642, 'total_flos': 2.764678008319181e+16, 'train_loss': 1.331983082371262, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Evaluation"
      ],
      "metadata": {
        "id": "v2GDZIDiA4IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(example):\n",
        "    inputs = torch.tensor(example['input_ids']).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0).to(device)\n",
        "\n",
        "    summary_ids = model.generate(inputs, attention_mask=attention_mask, max_length=128, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return {\"generated_summary\": summary}\n",
        "\n",
        "samsum_test_with_summaries = tokenized_samsum_test.map(generate_summary)\n",
        "xsum_test_with_summaries = tokenized_xsum_test_subset.map(generate_summary)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:02:23.291629Z",
          "iopub.execute_input": "2024-09-11T02:02:23.292021Z",
          "iopub.status.idle": "2024-09-11T02:18:55.348787Z",
          "shell.execute_reply.started": "2024-09-11T02:02:23.291985Z",
          "shell.execute_reply": "2024-09-11T02:18:55.347632Z"
        },
        "trusted": true,
        "id": "Bw-5jOgsA4IJ",
        "outputId": "e081e31e-1e36-42ae-fd5a-25343716d6e6",
        "colab": {
          "referenced_widgets": [
            "9ef4ba14e3b14be79d34c80cc8cf2ccd",
            "828dd379649b4dfbab1340fc86859473"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef4ba14e3b14be79d34c80cc8cf2ccd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/819 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "828dd379649b4dfbab1340fc86859473"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_62/1990374615.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs = torch.tensor(example['input_ids']).unsqueeze(0).to(device)\n/tmp/ipykernel_62/1990374615.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(example['attention_mask']).unsqueeze(0).to(device)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "\n",
        "rouge = load(\"rouge\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:23:24.094984Z",
          "iopub.execute_input": "2024-09-11T02:23:24.095698Z",
          "iopub.status.idle": "2024-09-11T02:23:25.165257Z",
          "shell.execute_reply.started": "2024-09-11T02:23:24.095656Z",
          "shell.execute_reply": "2024-09-11T02:23:25.164086Z"
        },
        "trusted": true,
        "id": "22jM0fX4A4IJ",
        "outputId": "02c956b8-b4a0-40db-a44b-ca1dd8c75666",
        "colab": {
          "referenced_widgets": [
            "241adac40649451883ef522106c30730"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "241adac40649451883ef522106c30730"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samsum_rouge_scores = rouge.compute(predictions=samsum_test_with_summaries[\"generated_summary\"], references=samsum_test_with_summaries[\"summary\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:25:36.656101Z",
          "iopub.execute_input": "2024-09-11T02:25:36.657008Z",
          "iopub.status.idle": "2024-09-11T02:25:38.296479Z",
          "shell.execute_reply.started": "2024-09-11T02:25:36.656964Z",
          "shell.execute_reply": "2024-09-11T02:25:38.29543Z"
        },
        "trusted": true,
        "id": "7ScsMB4uA4IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xsum_rouge_scores = rouge.compute(predictions=xsum_test_with_summaries[\"generated_summary\"], references=xsum_test_with_summaries[\"summary\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:25:46.921597Z",
          "iopub.execute_input": "2024-09-11T02:25:46.922257Z",
          "iopub.status.idle": "2024-09-11T02:25:48.869871Z",
          "shell.execute_reply.started": "2024-09-11T02:25:46.922218Z",
          "shell.execute_reply": "2024-09-11T02:25:48.868546Z"
        },
        "trusted": true,
        "id": "DYIcCqyfA4IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation Results on SAMSum Test Set:\")\n",
        "for key, value in samsum_rouge_scores.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:25:40.119982Z",
          "iopub.execute_input": "2024-09-11T02:25:40.120763Z",
          "iopub.status.idle": "2024-09-11T02:25:40.128497Z",
          "shell.execute_reply.started": "2024-09-11T02:25:40.120722Z",
          "shell.execute_reply": "2024-09-11T02:25:40.127326Z"
        },
        "trusted": true,
        "id": "h1upZsIrA4IJ",
        "outputId": "ae0169d3-7c4d-4e76-ce96-d4d3701e7604"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Evaluation Results on SAMSum Test Set:\nrouge1: 0.3988985196304326\nrouge2: 0.20096704267215054\nrougeL: 0.3050665531038751\nrougeLsum: 0.3052674111900092\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluation Results on XSum Test Subset:\")\n",
        "for key, value in xsum_rouge_scores.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:25:58.862886Z",
          "iopub.execute_input": "2024-09-11T02:25:58.863281Z",
          "iopub.status.idle": "2024-09-11T02:25:58.871326Z",
          "shell.execute_reply.started": "2024-09-11T02:25:58.863241Z",
          "shell.execute_reply": "2024-09-11T02:25:58.870317Z"
        },
        "trusted": true,
        "id": "wwoBSVgBA4IJ",
        "outputId": "df41697c-1da4-4f7d-f88d-7a6f6c270ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Evaluation Results on XSum Test Subset:\nrouge1: 0.20103645574255669\nrouge2: 0.03815603060235087\nrougeL: 0.13284899437336667\nrougeLsum: 0.13285161261780748\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_examples(dataset_with_summaries, num_examples=5):\n",
        "    for i in range(num_examples):\n",
        "        print(f\"Example {i+1}:\")\n",
        "        if 'dialogue' in dataset_with_summaries[i]:\n",
        "            print(f\"Dialogue: {dataset_with_summaries[i]['dialogue']}\")\n",
        "        print(f\"Reference Summary: {dataset_with_summaries[i]['summary']}\")\n",
        "        print(f\"Generated Summary: {dataset_with_summaries[i]['generated_summary']}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "print(\"SAMSum Test Set Examples:\")\n",
        "display_examples(samsum_test_with_summaries, num_examples=5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-11T02:29:33.152349Z",
          "iopub.execute_input": "2024-09-11T02:29:33.153332Z",
          "iopub.status.idle": "2024-09-11T02:29:33.181216Z",
          "shell.execute_reply.started": "2024-09-11T02:29:33.153289Z",
          "shell.execute_reply": "2024-09-11T02:29:33.18024Z"
        },
        "trusted": true,
        "id": "yYlJMfCjA4IJ",
        "outputId": "1df08beb-208d-48a8-d859-f490150d0ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "SAMSum Test Set Examples:\nExample 1:\nDialogue: Hannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him ğŸ™‚\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\nReference Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\nGenerated Summary: Larry called Betty last time they were at the park together. Hannah doesn't know Larry well. Amanda will text Larry.  .. Hannah and Amanda are going to meet at the end of the day. â€œHannahâ€™s not sure about it. \n--------------------------------------------------------------------------------\nExample 2:\nDialogue: Eric: MACHINE!\nRob: That's so gr8!\nEric: I know! And shows how Americans see Russian ;)\nRob: And it's really funny!\nEric: I know! I especially like the train part!\nRob: Hahaha! No one talks to the machine like that!\nEric: Is this his only stand-up?\nRob: Idk. I'll check.\nEric: Sure.\nRob: Turns out no! There are some of his stand-ups on youtube.\nEric: Gr8! I'll watch them now!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\nReference Summary: Eric and Rob are going to watch a stand-up on youtube.\nGenerated Summary: Rob and Eric like the Russian comedian's stand-up. Eric will watch some of his stand-ups on youtube now. Rob and Eric are laughing at the joke..    i'm going to watch some videos. i'm looking forward to it. \n--------------------------------------------------------------------------------\nExample 3:\nDialogue: Lenny: Babe, can you help me with something?\nBob: Sure, what's up?\nLenny: Which one should I pick?\nBob: Send me photos\nLenny:  <file_photo>\nLenny:  <file_photo>\nLenny:  <file_photo>\nBob: I like the first ones best\nLenny: But I already have purple trousers. Does it make sense to have two pairs?\nBob: I have four black pairs :D :D\nLenny: yeah, but shouldn't I pick a different color?\nBob: what matters is what you'll give you the most outfit options\nLenny: So I guess I'll buy the first or the third pair then\nBob: Pick the best quality then\nLenny: ur right, thx\nBob: no prob :)\nReference Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\nGenerated Summary: Lenny will buy the first or the third pair of purple trousers for Bob. He likes the first pair best, but Lenny already has two pairs of black ones. Lenny will pick the best quality of the trousers and they will give her the most outfit options.\n--------------------------------------------------------------------------------\nExample 4:\nDialogue: Will: hey babe, what do you want for dinner tonight?\nEmma:  gah, don't even worry about it tonight\nWill: what do you mean? everything ok?\nEmma: not really, but it's ok, don't worry about cooking though, I'm not hungry\nWill: Well what time will you be home?\nEmma: soon, hopefully\nWill: you sure? Maybe you want me to pick you up?\nEmma: no no it's alright. I'll be home soon, i'll tell you when I get home. \nWill: Alright, love you. \nEmma: love you too. \nReference Summary: Emma will be home soon and she will let Will know.\nGenerated Summary: Emma doesn't want to cook for dinner tonight. She will be home soon and will tell Will when she gets home. Will will pick her up if she wants to come home.   Â Emma and Will love each other very much, but Emma is not hungry. \n--------------------------------------------------------------------------------\nExample 5:\nDialogue: Ollie: Hi , are you in Warsaw\nJane: yes, just back! Btw are you free for diner the 19th?\nOllie: nope!\nJane: and the  18th?\nOllie: nope, we have this party and you must be there, remember?\nJane: oh right! i lost my calendar..  thanks for reminding me\nOllie: we have lunch this week?\nJane: with pleasure!\nOllie: friday?\nJane: ok\nJane: what do you mean \" we don't have any more whisky!\" lol..\nOllie: what!!!\nJane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\nOllie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\nJane: dont' worry, we'll check on friday.\nOllie: don't forget to bring some sun with you\nJane: I can't wait to be in Morocco..\nOllie: enjoy and see you friday\nJane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\nOllie: ok for tea!\nJane: I'm on my way..\nOllie: tea is ready, did you bring the pastries?\nJane: I already ate them all... see you in a minute\nOllie: ok\nReference Summary: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\nGenerated Summary: Jane is back in Warsaw. Ollie has a party on the 18th. They will meet on Friday at 6 pm for tea. Jane has already eaten the pastries.    Jane is going to Morocco.  She will bring some sun with her. \n--------------------------------------------------------------------------------\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Conclusion"
      ],
      "metadata": {
        "id": "XAXG0vshA4IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion:**\n",
        "\n",
        "The evaluation results demonstrate a significant improvement in the model's performance on dialogue summarization after fine-tuning on the SAMSum dataset. The ROUGE-1 score increased to 0.3989, and the ROUGE-2 score reached 0.2009, reflecting the model's enhanced ability to capture key information in dialogue-based content. This improvement underscores the effectiveness of domain-specific fine-tuning in enhancing summarization quality for dialogues.\n",
        "\n",
        "However, the model showed only marginal improvement on the out-of-domain XSum test subset, with ROUGE-1 and ROUGE-2 scores remaining relatively low at 0.2010 and 0.0382, respectively. This suggests that while the model's performance in dialogue summarization improved significantly, its ability to generalize to more extreme extractive summarization tasks, like those presented in XSum, remains limited.\n",
        "\n",
        "Overall, these results highlight the importance of fine-tuning for domain-specific tasks and the challenges associated with cross-domain generalization in text summarization models."
      ],
      "metadata": {
        "id": "rlM9Ne1PA4IJ"
      }
    }
  ]
}